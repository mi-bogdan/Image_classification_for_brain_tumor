<h2 align="center">Прогнозирование опухолей мозга</h2>

**Ссылки**:
- [Вконтакте](https://vk.com/id404101172) 
- [Telegram](https://t.me/bogdan_shnyra)

## Описание

#####  Набор данных дляк лассификации опухолей мозга:
    https://huggingface.co/datasets/sartajbhuvaji/Brain-Tumor-Classification

Представляет собой коллекцию изображений, которые используются для 
обучения машинных и глубоких моделей обучения для классификации 
опухолей мозга.

Характеристики набора данных:

- Задача: Классификация
- Тип данных: Изображения

Количество классов: Набор включают категории (glioma_tumor, 
meningioma_tumor, no_tumor, pituitary_tumor).
Метод опорных векторов (SVM, Support Vector Machines) — это 
мощный алгоритм машинного обучения для задач классификации, регрессии 
и детекции выбросов. В контексте бинарной классификации, основная идея 
SVM заключается в нахождении гиперплоскости, которая наилучшим 
образом разделяет два класса в векторном пространстве.
Рассмотрим два класса, обозначенные метками \(yi = \pm 1\), где \(yi\) 
— это класс для \(i\)-го примера обучающего набора с вектором признаков 
\(xi\). Гиперплоскость, которая их разделяет, может быть описана 
уравнением:
\[w^T x + b = 0\]
где \(w\) — вектор весов, перпендикулярный гиперплоскости; \(x\) — 
вектор входных признаков; \(b / \|w\|\) определяет смещение от начала 
координат до гиперплоскости.
Основная цель состоит в максимизации ширины маргинала между 
двумя классами, что эквивалентно минимизации \(1/2 \|w\|^2\) с 
ограничениями:
\[yi (w^T xi + b) \geq 1, \quad \forall i\]
Это условие гарантирует, что все точки обучающего набора находятся 
за пределами маргинала или на его границе. Решение данной задачи 
оптимизации даёт \(w\) и \(b\), которые определяют оптимальную 
разделяющую гиперплоскость.
Для решения этой задачи оптимизации часто используют метод 
Лагранжевых множителей, преобразовав её в двойственную задачу:
\[L(w, b, \alpha) = 1/2 \|w\|^2 - \sum{i=1}^m \alphai [yi (w^T xi + b) - 1]\]
где \(\alphai\) — Лагранжевы множители, \(m\) — количество 
обучающих примеров.
Задача оптимизации заключается в минимизации \(L\) по \(w\) и \(b\), и 
одновременной максимизации \(L\) по \(\alphai\), при этом \(\alphai \geq 0\). 
Решение представляет собой выборку векторов \(xi\), для которых \(\alphai > 
0\), называемых опорными векторами.
Когда данные не линейно разделимы в исходном пространстве 
признаков, SVM использует так называемые "ядровые трюки", позволяющие 
выполнять классификацию в пространстве более высокой размерности путем 
применения ядровой функции \(K(xi, xj) = \phi(xi)^T \phi(xj)\), где \(\phi(xi)\) 
— это отображение вектора признаков \(xi\) в пространство более высокой 
размерности.
Таким образом, двойственная задача в терминах ядра принимает вид:
[\max{\alpha} \left[ \sum{i=1}^m \alphai - 1/2 \sum{i=1}^m \sum{j=1}^m 
\alphai \alphaj yi yj K(xi, xj) \right]\] 
с ограничениями:
\[\sum{i=1}^m \alphai yi = 0, \quad \alphai \geq 0, \quad \forall i\]
Где решение задачи дает набор \(\alphai\), а классификация новых точек 
данных \(x\) производится путем вычисления знака функции принятия 
решения:
[f(x) = \text{sgn}\left(\sum{i=1}^m \alphai yi K(xi, x) + b\right)\]

Детектор SIFT (Scale-Invariant Feature Transform) - это алгоритм в 
области компьютерного зрения, разработанный Дэвидом Лоу (David Lowe) в 
2004 году для выявления и описания локальных особенностей изображений. 
Процесс SIFT можно разбить на несколько ключевых шагов, каждый из 
которых представляет собой серию математических и алгоритмических 
операций.
1. Построение гауссова пирамидального масштабного пространства 
(Scale-space Extrema Detection)
Для каждого изображения строится "пирамида" изображений, где 
каждый последующий слой представляет собой уменьшенную и размытую 
версию предыдущего. Для получения размытых изображений применяется 
гауссово размытие. После построения пирамиды, для каждой точки на 
изображении проверяется, является ли она экстремумом (максимумом или 
минимумом) по сравнению с её соседями в масштабном пространстве:
\[L(x, y, \sigma) = G(x, y, \sigma) * I(x, y)\]
где \(L(x, y, \sigma)\) — это масштабно-пространственная функция, 
получаемая сверткой исходного изображения \(I(x, y)\) с Гауссовым ядром 
\(G(x, y, \sigma)\) с обозначением масштаба \(\sigma\).
2. Локализация ключевых точек (Key-point Localization)
В этом шаге выполняется точная локализация ключевых точек путём 
уточнения их местоположения и масштаба. Это достигается путем 
устранения точек с низкой контрастностью и удаления точек, 
расположенных по краям. Для этого используется вторая производная 
функции масштабного пространства, представляемая в форме матрицы Гессе:
\[H = \begin{bmatrix} D_{xx} & D_{xy} \\ D_{yx} & D_{yy} 
\end{bmatrix}\]
3. Назначение ориентации
Для каждой ключевой точки определяется одна или несколько 
ориентаций на основе локальных градиентов изображений. Это делает 
описание ключевой точки инвариантным к повороту. Градиенты могут быть 
выражены как:
\[m(x,y) = \sqrt{(L(x+1, y) - L(x-1, y))^2 + (L(x, y+1) - L(x, y-1))^2}\]
\[\theta(x,y) = \tan^{-1}\left(\frac{L(x, y+1) - L(x, y-1)}{L(x+1, y) - L(x-1, 
y)}\right)\]
где \(m(x,y)\) и \(\theta(x,y)\) обозначают величину и направление 
градиента в точке \((x,y)\).

Параметры обучения включают в себя настройки алгоритма, извлечения дескрипторов (SIFT) и классификатора 
(SVM), а также параметры разделения данных на обучающую и тестовую 
выборки и процесс оценки модели:
1. SVC (Support Vector Classifier):
   - kernel='rbf' - выбор ядра для SVM (указывает на использование 
радиально-базисной функции в качестве ядра). Выбор ядра определяет вид 
пространства, в котором будут искаться разделяющие гиперплоскости между 
классами.
   - Параметры StandardScaler() в конвейере (make_pipeline) указывают 
на использование стандартизации признаков перед обучением SVM. Это 
гарантирует, что все признаки имеют среднее значение 0 и стандартное 
отклонение 1.
2. traintestsplit:
 - test_size=0.3 - процентное соотношение данных, отводимых под 
тестовую выборку. В данном случае 30% данных используются для 
тестирования, а 70% - для обучения.
   - random_state=42 - параметр для установки начального числа 
генератора случайных чисел, что обеспечивает воспроизводимость 
результатов при разделении данных на обучающую и тестовую выборки.

## Функционал

- Прогнозирование опухолей мозга

### Инструменты разработки

**Стек:**
- Python 
- Docker
- streamlit
- scikit-learn

## Разработка

##### 1) Клонировать репозиторий

    git clone https://github.com/mi-bogdan/Image_classification_for_brain_tumor.git

##### 2) Создать виртуальное окружение

    python -m venv venv
    
##### 3) Активировать виртуальное окружение

    venv/Scripts/activate       

##### 4) Устанавливить зависимости:

    pip install -r requirements.txt

##### 5) Загрузить датасет по ссылке выше папку (Training):

##### 6) Обучить модель:
    python models_classification_brain_tumor.py

##### 7) Запустить интерфецс для работы:
    python streamlit run main.py

Copyright (c) 2024-present, - Shnyra Bogdan
