<h2 align="center">Прогнозирование опухолей мозга</h2>

**Ссылки**:
- [VK](https://vk.com/id404101172)   

## Описание

Наборданныхдляклассификацииопухолеймозга:
#####  DataSet
    https://huggingface.co/datasets/sartajbhuvaji/Brain-Tumor-Classification

представляет собой коллекцию изображений, которые используются для 
обучения машинных и глубоких моделей обучения для классификации 
опухолей мозга.
Характеристики набора данных:
- Задача: Классификация
- Тип данных: Изображения
- Количество классов: Набор включают категории (glioma_tumor, 
meningioma_tumor, no_tumor, pituitary_tumor).
Метод опорных векторов (SVM, Support Vector Machines) — это 
мощный алгоритм машинного обучения для задач классификации, регрессии 
и детекции выбросов. В контексте бинарной классификации, основная идея 
SVM заключается в нахождении гиперплоскости, которая наилучшим 
образом разделяет два класса в векторном пространстве.
Рассмотрим два класса, обозначенные метками \(yi = \pm 1\), где \(yi\) 
— это класс для \(i\)-го примера обучающего набора с вектором признаков 
\(xi\). Гиперплоскость, которая их разделяет, может быть описана 
уравнением:
\[w^T x + b = 0\]
где \(w\) — вектор весов, перпендикулярный гиперплоскости; \(x\) — 
вектор входных признаков; \(b / \|w\|\) определяет смещение от начала 
координат до гиперплоскости.
Основная цель состоит в максимизации ширины маргинала между 
двумя классами, что эквивалентно минимизации \(1/2 \|w\|^2\) с 
ограничениями:
\[yi (w^T xi + b) \geq 1, \quad \forall i\]
Это условие гарантирует, что все точки обучающего набора находятся 
за пределами маргинала или на его границе. Решение данной задачи 
оптимизации даёт \(w\) и \(b\), которые определяют оптимальную 
разделяющую гиперплоскость.
Для решения этой задачи оптимизации часто используют метод 
Лагранжевых множителей, преобразовав её в двойственную задачу:
\[L(w, b, \alpha) = 1/2 \|w\|^2 - \sum{i=1}^m \alphai [yi (w^T xi + b) - 1]\]
где \(\alphai\) — Лагранжевы множители, \(m\) — количество 
обучающих примеров.
Задача оптимизации заключается в минимизации \(L\) по \(w\) и \(b\), и 
одновременной максимизации \(L\) по \(\alphai\), при этом \(\alphai \geq 0\). 
Решение представляет собой выборку векторов \(xi\), для которых \(\alphai > 
0\), называемых опорными векторами.
Когда данные не линейно разделимы в исходном пространстве 
признаков, SVM использует так называемые "ядровые трюки", позволяющие 
выполнять классификацию в пространстве более высокой размерности путем 
применения ядровой функции \(K(xi, xj) = \phi(xi)^T \phi(xj)\), где \(\phi(xi)\) 
— это отображение вектора признаков \(xi\) в пространство более высокой 
размерности.
Таким образом, двойственная задача в терминах ядра принимает вид:
[\max{\alpha} \left[ \sum{i=1}^m \alphai - 1/2 \sum{i=1}^m \sum{j=1}^m 
\alphai \alphaj yi yj K(xi, xj) \right]\] 
с ограничениями:
\[\sum{i=1}^m \alphai yi = 0, \quad \alphai \geq 0, \quad \forall i\]
Где решение задачи дает набор \(\alphai\), а классификация новых точек 
данных \(x\) производится путем вычисления знака функции принятия 
решения:
[f(x) = \text{sgn}\left(\sum{i=1}^m \alphai yi K(xi, x) + b\right)\]


Детектор SIFT (Scale-Invariant Feature Transform) - это алгоритм в 
области компьютерного зрения, разработанный Дэвидом Лоу (David Lowe) в 
2004 году для выявления и описания локальных особенностей изображений. 
Процесс SIFT можно разбить на несколько ключевых шагов, каждый из 
которых представляет собой серию математических и алгоритмических 
операций.
1. Построение гауссова пирамидального масштабного пространства 
(Scale-space Extrema Detection)
Для каждого изображения строится "пирамида" изображений, где 
каждый последующий слой представляет собой уменьшенную и размытую 
версию предыдущего. Для получения размытых изображений применяется 
гауссово размытие. После построения пирамиды, для каждой точки на 
изображении проверяется, является ли она экстремумом (максимумом или 
минимумом) по сравнению с её соседями в масштабном пространстве:
\[L(x, y, \sigma) = G(x, y, \sigma) * I(x, y)\]
где \(L(x, y, \sigma)\) — это масштабно-пространственная функция, 
получаемая сверткой исходного изображения \(I(x, y)\) с Гауссовым ядром 
\(G(x, y, \sigma)\) с обозначением масштаба \(\sigma\).
2. Локализация ключевых точек (Key-point Localization)
В этом шаге выполняется точная локализация ключевых точек путём 
уточнения их местоположения и масштаба. Это достигается путем 
устранения точек с низкой контрастностью и удаления точек, 
расположенных по краям. Для этого используется вторая производная 
функции масштабного пространства, представляемая в форме матрицы Гессе:
\[H = \begin{bmatrix} D_{xx} & D_{xy} \\ D_{yx} & D_{yy} 
\end{bmatrix}\]
3. Назначение ориентации
Для каждой ключевой точки определяется одна или несколько 
ориентаций на основе локальных градиентов изображений. Это делает 
описание ключевой точки инвариантным к повороту. Градиенты могут быть 
выражены как:
\[m(x,y) = \sqrt{(L(x+1, y) - L(x-1, y))^2 + (L(x, y+1) - L(x, y-1))^2}\]
\[\theta(x,y) = \tan^{-1}\left(\frac{L(x, y+1) - L(x, y-1)}{L(x+1, y) - L(x-1, 
y)}\right)\]
где \(m(x,y)\) и \(\theta(x,y)\) обозначают величину и направление 
градиента в точке \((x,y)\).

Параметры обучения включают в себя настройки алгоритма 
кластеризации (KMeans), извлечения дескрипторов (SIFT) и классификатора 
(SVM), а также параметры разделения данных на обучающую и тестовую 
выборки и процесс оценки модели:
1. KMeans:
   - n_clusters=100 - количество кластеров для метода KMeans, которое 
напрямую влияет на размер получаемого "мешка визуальных слов". Число 
кластеров определяет, на сколько категорий будут делиться все дескрипторы 
SIFT всех изображений на этапе построения признаков.
2. SVC (Support Vector Classifier):
   - kernel='rbf' - выбор ядра для SVM (указывает на использование 
радиально-базисной функции в качестве ядра). Выбор ядра определяет вид 
пространства, в котором будут искаться разделяющие гиперплоскости между 
классами.
   - Параметры StandardScaler() в конвейере (make_pipeline) указывают 
на использование стандартизации признаков перед обучением SVM. Это 
гарантирует, что все признаки имеют среднее значение 0 и стандартное 
отклонение 1.
3. traintestsplit:
 - test_size=0.3 - процентное соотношение данных, отводимых под 
тестовую выборку. В данном случае 30% данных используются для 
тестирования, а 70% - для обучения.
   - random_state=42 - параметр для установки начального числа 
генератора случайных чисел, что обеспечивает воспроизводимость 
результатов при разделении данных на обучающую и тестовую выборки.

## Функционал

- Ответ на сообщения пользователя его же текстом.
- Простая структура кода для легкости понимания и изменения.


### Инструменты разработки

**Стек:**
- Python 
- aiogram

## Разработка

##### 1) Клонировать репозиторий

    git clone https://github.com/mi-bogdan/telegram_echo_bot.git

##### 2) Создать виртуальное окружение

    python -m venv venv
    
##### 3) Активировать виртуальное окружение

    venv/Scripts/activate       

##### 4) Устанавливить зависимости:

    pip install -r requirements.txt

##### 5) Запустить проект:


https://huggingface.co/datasets/sartajbhuvaji/Brain-Tumor-Classification